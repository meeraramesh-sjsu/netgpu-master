/* 
	MODULE:Histograms 
	TYPE: Analysis

	PrePreprocessor orders (ppp.sh): 

	###PATTERNS $HISTOGRAMS$ANALYSIS();

*/

//#warning "Histogram Analysis"

#ifndef libAnalysisHistograms_h 
#define libAnalysisHistograms_h 

typedef struct{
	ANALYSIS_INPUT_TYPE user;
	float counter;
}COMPOUND_NAME(ANALYSIS_NAME,histogramAnalysis_t);

#endif  //libAnalysisHistograms

/* Redefine TYPES */
#undef  ANALYSIS_INPUT_TYPE
#define ANALYSIS_INPUT_TYPE COMPOUND_NAME(ANALYSIS_NAME,histogramAnalysis_t)

#ifdef __CUDACC__

/***** HISTOGRAM ANALYSIS *****/
#define $HISTOGRAMS$ANALYSIS() \
	COMPOUND_NAME(ANALYSIS_NAME,preDefinedAnalysisCodeHistogram)(GPU_buffer,GPU_data,GPU_results,state);\
	__syncthreads()

#define SHARED_BUFFER_SIZE MAX_BUFFER_PACKETS/ANALYSIS_TPB

template<typename T,typename R> __device__ __inline__ void COMPOUND_NAME(ANALYSIS_NAME,preDefinedAnalysisCodeHistogram)(packet_t* GPU_buffer, T* GPU_data, R* GPU_results, analysisState_t state){ 

	/*Count elements inside block */
	int i,counter,futurePosition;
	T myValue;

	__shared__ T elements[ANALYSIS_TPB];
	__shared__ bool leaders[ANALYSIS_TPB];

	state.blockIterator = blockIdx.x;
	//Looping when array is larger than grid dimensions 
	while( state.blockIterator < state.windowState.totalNumberOfBlocks ){

		//Mine to shared
		elements[threadIdx.x] = cudaSafeGet(&GPU_data[state.blockIterator*blockDim.x+threadIdx.x]); 

		//initialize leader flags 
		if(cudaSharedIsNotNull(&elements[threadIdx.x].user)){
			leaders[threadIdx.x] = true;
			if(elements[threadIdx.x].counter == 0)
				elements[threadIdx.x].counter = 1;
		}else	
			leaders[threadIdx.x] = false;
	
		myValue = elements[threadIdx.x];	
		SYNCTHREADS();
	
		if(leaders[threadIdx.x]){
			
			//Calculate nº of elements like mine
			for(i=0,counter=0;i<blockDim.x;i++){
				if(cudaSharedMemcmp(&elements[threadIdx.x].user,&elements[i].user) == 0){
					if(i<threadIdx.x)
						leaders[threadIdx.x] = false;

					counter+=elements[i].counter;
	
				}
			
			}
		}
		SYNCTHREADS();
	

		if(leaders[threadIdx.x]){
			for(i=(threadIdx.x-1),futurePosition=0;i>=0;i--){
				if(leaders[i])
					futurePosition++;	
			}		
		
			//fprintf(stderr,"blockIterator: %d, futurePosition: %d -> threadIdx.x: %d\n",state.blockIterator,futurePosition,threadIdx.x);
			myValue.counter = counter;
			GPU_data[futurePosition+state.blockIterator*blockDim.x] = myValue;
	
		}

		if(threadIdx.x == 0){
			//Calculate number of elements en each block and place it in Position nº 0 of each block on results vector
			for(i=0,counter=0;i<blockDim.x;i++){
				if(leaders[i])
					counter++;				
			}
			state.GPU_auxBlocks[state.blockIterator] = counter;
			

		}

		state.blockIterator += gridDim.x;
		SYNCTHREADS();
	}

	/*** Sync ALL BLOCKS ***/
	#include "../PrecodedSyncblocks.def"

	int counter, i,blockIndex, futurePosition;	
	blockIndex = blockIdx.x;

	T myValue;	
	__shared__ bool leaders[ANALYSIS_TPB];
	__shared__ T blockElements[ANALYSIS_TPB];
	__shared__ int numOfElementsPerBlock[SHARED_BUFFER_SIZE]; 
	__shared__ int step;
	state.blockIterator = blockIdx.x;

	//Looping when array is larger than grid dimensions 
	while( state.blockIterator < state.windowState.totalNumberOfBlocks ){
	
		step = 0;
		//Mine my block items, num of elements

		if(threadIdx.x == 0){
			numOfElementsPerBlock[0] = state.GPU_auxBlocks[state.blockIterator];
		}
		SYNCTHREADS();

		if(threadIdx.x < numOfElementsPerBlock[0]){ //Save as many global memory accesses as possible
			blockElements[threadIdx.x] = cudaSafeGet(&GPU_data[state.blockIterator*blockDim.x+threadIdx.x]);
		}else{
			cudaSharedMemset(&blockElements[threadIdx.x],0);	
		}

		//Set leader flags appropiate to content of myValue	
		if(cudaSharedIsNotNull(&blockElements[threadIdx.x].user))
			leaders[threadIdx.x] = true;
		else
			leaders[threadIdx.x] = false;
	
		//Set myValue and counters
		myValue = blockElements[threadIdx.x];	
		counter = blockElements[threadIdx.x].counter;	


		//Loop for all blocks
		for(blockIndex = 0;blockIndex<state.windowState.totalNumberOfBlocks;blockIndex++){	
			
			//Do not erase this	
			SYNCTHREADS();

			if(blockIndex == state.blockIterator)
				continue;
			
			bool hasLeaders = false;
			for(i=0;i<blockDim.x;i++){
				if(leaders[i]){
					hasLeaders = true;
					break;
				}
			} 
		
			if(!hasLeaders)
				break;

			//Mine to numOfElements 
			if(blockIndex >= (step*(SHARED_BUFFER_SIZE))){
			
				if(threadIdx.x < (SHARED_BUFFER_SIZE)){
					numOfElementsPerBlock[threadIdx.x] = state.GPU_auxBlocks[(step*(SHARED_BUFFER_SIZE))+threadIdx.x];
				}
			}	
			SYNCTHREADS();

			if(blockIndex >= (step*(SHARED_BUFFER_SIZE)) && threadIdx.x == 0){
					step++;
			
			}
			SYNCTHREADS();

			//Mine blockElements 
			if(threadIdx.x < numOfElementsPerBlock[blockIndex - ((step-1)*(SHARED_BUFFER_SIZE))]){
				blockElements[threadIdx.x] = cudaSafeGet(&GPU_data[blockIndex*blockDim.x+threadIdx.x]);
			}else{
				cudaSharedMemset(&blockElements[threadIdx.x],0);	
			}
			SYNCTHREADS();

			if(leaders[threadIdx.x]){

				for(i=0;i<numOfElementsPerBlock[blockIndex - ((step-1)*(SHARED_BUFFER_SIZE))];i++){

					if(cudaSharedMemcmp(&myValue.user,&blockElements[i].user) == 0){ //WARN: COMAPRING NOT SHARED ELEMENT
					
						if(blockIndex> state.blockIterator)
							counter += blockElements[i].counter;
						else
							leaders[threadIdx.x] = false;
					}	
				}
	
			}

		}

		SYNCTHREADS();

		/* Determine future position inside block*/
		if(leaders[threadIdx.x]){
			for(i=(threadIdx.x-1),futurePosition=0;i>=0;i--){
				if(leaders[i])
					futurePosition++;	
			}		
	
		}

		if(leaders[threadIdx.x]){
				myValue.counter = counter;	
				((T*)state.GPU_aux)[state.blockIterator*blockDim.x+futurePosition]= myValue;
		}

		if(threadIdx.x == 0){
			for(i=0,counter=0;i<blockDim.x;i++){
				if(leaders[i])
					counter++;
			}
			state.GPU_auxBlocks[state.windowState.totalNumberOfBlocks+state.blockIterator] = counter;

		
		}	

		SYNCTHREADS();
		
		//Iterate
		state.blockIterator += gridDim.x;
	}


	/*** Sync ALL BLOCKS ***/
	#include "../PrecodedSyncblocks.def"

	//This kernel condenses values at the begining of the array
	bool isLeader;	
	int i, futurePosition,step,totalNumberOfElements;
	__shared__ int numOfElementsPerBlock[SHARED_BUFFER_SIZE]; 

	state.blockIterator = blockIdx.x;

	//Looping when array is larger than grid dimensions 
	while( state.blockIterator < state.windowState.totalNumberOfBlocks ){
		isLeader=false;
		totalNumberOfElements=0;
		futurePosition=0;
		
		for(step=0;(step*(SHARED_BUFFER_SIZE))<state.windowState.totalNumberOfBlocks;step++){
		
			//Mines piece of numOfElementsPerBlock
			if(threadIdx.x < (SHARED_BUFFER_SIZE)){
				numOfElementsPerBlock[threadIdx.x] = state.GPU_auxBlocks[state.windowState.totalNumberOfBlocks+(step*(SHARED_BUFFER_SIZE))+threadIdx.x];
			}
			SYNCTHREADS();
				
			//Increments the total number of blocks counter and futurePosition and modifies isLeader flag if thread value is not null	
			for(i=0;i<SHARED_BUFFER_SIZE;i++){
				if(((step*(SHARED_BUFFER_SIZE))+i)<state.blockIterator)
					futurePosition += numOfElementsPerBlock[i];

				totalNumberOfElements += numOfElementsPerBlock[i];

				if((((step*(SHARED_BUFFER_SIZE))+i)== state.blockIterator) && (threadIdx.x < numOfElementsPerBlock[i])){
					isLeader = true;
				}
			}
			SYNCTHREADS();
		}

		if(isLeader){
	
			//Increments offset by threadIdx.x position (it's own block offset)
			futurePosition += threadIdx.x;
			GPU_results[futurePosition] = ((T*)state.GPU_aux)[(state.blockIterator*blockDim.x+threadIdx.x)];
		}

		if((state.blockIterator*blockDim.x+threadIdx.x) == 0){
			//Set nº of elements at the output
			state.GPU_auxBlocks[0] = totalNumberOfElements;

			//Set to 0 rest of the blocks (not really necessary)
			for(i=1;i<state.windowState.totalNumberOfBlocks;i++){
				state.GPU_auxBlocks[i] = 0; 
			}
		}
		SYNCTHREADS();
		//Iterate

		state.blockIterator += gridDim.x;
	}

	/*** Sync ALL BLOCKS ***/
	#include "../PrecodedSyncblocks.def"

	SYNCTHREADS();	
	#if __SYNCBLOCKS_COUNTER == 0
		COMPOUND_NAME(ANALYSIS_NAME,operations)(GPU_buffer, GPU_data, GPU_results,state);
	#endif

}


/***** END OF HISTOGRAM ANALYSIS *****/


#endif //__CUDACC__

/* Redefine DATA_ELEMENT and RESULT_ELEMENT */
#undef DATA_ELEMENT
#define DATA_ELEMENT GPU_data[POS].user 

#undef RESULT_ELEMENT
#define RESULT_ELEMENT GPU_results[POS].user 



