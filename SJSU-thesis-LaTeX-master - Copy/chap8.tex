\chapter{Conclusion}

In this thesis, I have presented a framework that utilizes the GPU for header checking and pattern matching. The classic Rabin-Karp algorithm, the Aho-Corasick algorithm, and the Wu-Manber algorithm were parallelized using CUDA to run on the GPU to perform pattern matching. Parallelism approaches explored were 1) thread-level parallelism, in which each thread analyzes a single packet and 2) block-level parallelism, in which a block of threads analyze a single packet. In my evaluation, block-level parallelism outperformed thread-level parallelism by a factor of 116, and the Aho-Corasick and the Wu-Manber algorithms outperformed the Rabin-Karp algorithm for multi-pattern matching. Furthermore, the GPU-version IDS outperformed at least 60 times over the CPU-version. Aho-Corasick algorithm performed better than the Wu-Manber algorithm in terms of shared memory efficiency, warp efficiency, SM utilization and pipeline stalls due to memory throttle. The Wu-Manber algorithm performed better than the Aho-Corasick algorithm in terms of global memory usage. The Rabin-Karp algorithm performed better than the other two algorithms with respect to SM utilization and memory throttle.

Global memory access efficiency was higher in the Rabin-Karp algorithm because the memory accesses are highly coalesced. The global memory usage for the Aho-Corasick algorithm was 10x and 1000x greater than the Wu-Manber algorithm and the Rabin-Karp algorithm, respectively. Because the Aho-Corasick and the Wu-Manber algorithms store large tables generated during the pre-processing stage in global memory. Based on the evaluation results, I conclude that the Aho-Corasick algorithm is ideal for DPI engines with a large number of patterns, the Wu-Manber algorithm works well for engines which require low memory consumption and the Rabin-Karp algorithm is suitable for a DPI engine which requires highly coalesced memory accesses. 

In the future, I plan on applying these algorithms to FPGAs and compare the performance between the GPU and the FPGA. In the Aho-Corasick algorithm, there is an overhead associated with the amount of global memory used. To solve this, I plan to use multiple GPUs and break the FSM into multiple chunks and allocate each chunk to a GPU. Thus, I would be able to reduce the total global memory usage while decreasing the number of pipeline stalls that occurred due to memory latency. This would then lead to multiple gigabit per second throughput. 